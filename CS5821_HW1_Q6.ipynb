{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6IdK3hdo7ON"
      },
      "source": [
        "# **Advanced DNN with Callbacks and Adaptive Learning Rate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9UPqJgko719"
      },
      "source": [
        "*Objective: Develop an advanced DNN with sophisticated features such as callbacks and adaptive learning rates for enhanced performance and stability*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-Kv97GlpEW2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Embedding, LSTM, GRU, Dense, Bidirectional, Input, Attention, Layer)\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgcC0mmFpECV"
      },
      "source": [
        "## **Load Dataset and Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTIvthp3rIrF",
        "outputId": "92b3fd6d-c182-41f6-c28e-cf0c9b09d38c"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000  \n",
        "max_sequence_length = 200  \n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Testing samples: {len(x_test)}\")\n",
        "\n",
        "x_train_padded = pad_sequences(x_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "x_test_padded = pad_sequences(x_test, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(f\"Padded training shape: {x_train_padded.shape}\")\n",
        "print(f\"Padded testing shape: {x_test_padded.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFGVSvCfrWjC"
      },
      "outputs": [],
      "source": [
        "def build_base_model(vocab_size, max_sequence_length, embedding_dim=128, lstm_units=64):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "        LSTM(lstm_units),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "base_model = build_base_model(vocab_size, max_sequence_length)\n",
        "base_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "base_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QBXz1Wb36t9L",
        "outputId": "3f36e399-b0dc-4900-96da-1dbbcefe3052"
      },
      "outputs": [],
      "source": [
        "def build_bidirectional_model(vocab_size, max_sequence_length, embedding_dim=128, lstm_units=64):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "        Bidirectional(LSTM(lstm_units)),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "bidirectional_model = build_bidirectional_model(vocab_size, max_sequence_length)\n",
        "bidirectional_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bidirectional_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyBq27i6863G"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1),\n",
        "                                 initializer='random_normal', trainable=True)\n",
        "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1),\n",
        "                                 initializer='zeros', trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute attention scores\n",
        "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        # Compute context vector\n",
        "        context_vector = attention_weights * inputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector\n",
        "\n",
        "def build_attention_model(vocab_size, max_sequence_length, embedding_dim=128, lstm_units=64):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length)(inputs)\n",
        "    lstm_out = LSTM(lstm_units, return_sequences=True)(embedding)\n",
        "    attention = AttentionLayer()(lstm_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(attention)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "attention_model = build_attention_model(vocab_size, max_sequence_length)\n",
        "attention_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "attention_model.summary()\n",
        "\n",
        "def build_bidirectional_attention_model(vocab_size, max_sequence_length, embedding_dim=128, lstm_units=64):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length)(inputs)\n",
        "    bidirectional_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedding)\n",
        "    attention = AttentionLayer()(bidirectional_lstm)\n",
        "    outputs = Dense(1, activation='sigmoid')(attention)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "bidirectional_attention_model = build_bidirectional_attention_model(vocab_size, max_sequence_length)\n",
        "bidirectional_attention_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bidirectional_attention_model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TENDbxu5sO1F"
      },
      "source": [
        "## **Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRTvGAO_sJ8j"
      },
      "outputs": [],
      "source": [
        "# Common training parameters\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "validation_split = 0.2\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# Function to train a model\n",
        "def train_model(model, x_train, y_train):\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=2\n",
        "    )\n",
        "    return history\n",
        "\n",
        "# Train all models\n",
        "print(\"Training Base Model...\")\n",
        "base_history = train_model(base_model, x_train_padded, y_train)\n",
        "\n",
        "print(\"\\nTraining Bidirectional Model...\")\n",
        "bidirectional_history = train_model(bidirectional_model, x_train_padded, y_train)\n",
        "\n",
        "print(\"\\nTraining Attention Model...\")\n",
        "attention_history = train_model(attention_model, x_train_padded, y_train)\n",
        "\n",
        "print(\"\\nTraining Bidirectional Attention Model...\")\n",
        "bidirectional_attention_history = train_model(bidirectional_attention_model, x_train_padded, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuUbriactwCW"
      },
      "outputs": [],
      "source": [
        "def get_callbacks(model_name):\n",
        "    early_stop = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    checkpoint = callbacks.ModelCheckpoint(\n",
        "        filepath=f'{model_name}.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True\n",
        "    )\n",
        "\n",
        "    lr_reduce = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.1,\n",
        "        patience=5,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return [early_stop, checkpoint, lr_reduce]\n",
        "\n",
        "def compile_and_train(model, model_name, train_gen, val_gen, epochs=100):\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    cb = get_callbacks(model_name)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=cb,\n",
        "        steps_per_epoch=len(train_gen),\n",
        "        validation_steps=len(val_gen),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time for {model_name}: {training_time:.2f} seconds\")\n",
        "\n",
        "    return history, training_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAnGs54cvb-h"
      },
      "source": [
        "## **Define Callbacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJh5L9Ccvf4e",
        "outputId": "9487d10c-48dc-4eea-91bc-6231bed877b4"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, model_name):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot histories\n",
        "plot_history(base_history, \"Base Model\")\n",
        "plot_history(bidirectional_history, \"Bidirectional Model\")\n",
        "plot_history(attention_history, \"Attention Model\")\n",
        "plot_history(bidirectional_attention_history, \"Bidirectional Attention Model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0lSF8oD7HxP"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, x_test, y_test, model_name):\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"{model_name} - Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "    return loss, accuracy\n",
        "\n",
        "print(\"\\nEvaluating Models on Test Data:\")\n",
        "\n",
        "base_loss, base_acc = evaluate_model(base_model, x_test_padded, y_test, \"Base Model\")\n",
        "bidirectional_loss, bidirectional_acc = evaluate_model(bidirectional_model, x_test_padded, y_test, \"Bidirectional Model\")\n",
        "attention_loss, attention_acc = evaluate_model(attention_model, x_test_padded, y_test, \"Attention Model\")\n",
        "bidirectional_attention_loss, bidirectional_attention_acc = evaluate_model(bidirectional_attention_model, x_test_padded, y_test, \"Bidirectional Attention Model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sENkaveU7KAB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Model': [\n",
        "        'Base Model',\n",
        "        'Bidirectional Model',\n",
        "        'Attention Model',\n",
        "        'Bidirectional Attention Model'\n",
        "    ],\n",
        "    'Test Loss': [\n",
        "        base_loss,\n",
        "        bidirectional_loss,\n",
        "        attention_loss,\n",
        "        bidirectional_attention_loss\n",
        "    ],\n",
        "    'Test Accuracy': [\n",
        "        base_acc,\n",
        "        bidirectional_acc,\n",
        "        attention_acc,\n",
        "        bidirectional_attention_acc\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(data)\n",
        "print(\"\\nPerformance Summary:\")\n",
        "print(results_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
